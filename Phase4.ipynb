{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Test_Phase.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"b6ro3GJZHPNc"},"source":["**Data loading and Preprocessing**\n","\n","\n","*   We will load all the files directly from the Google drive. So first we have to mount the drive into collab notebook.\n","\n","*   For preprocessing we will only use csv module of python. \n"]},{"cell_type":"code","metadata":{"id":"91gV0HOvHlk_","executionInfo":{"status":"ok","timestamp":1620207220997,"user_tz":-330,"elapsed":6587,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["import csv\n","import string\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","#setting the device to \"cuda\" if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QEnd71w8IW_5"},"source":["Mounting the google drive\n","\n","- Upload \"train.csv\" and \"testhindistatements.csv\" inside the collab folder in google drive"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O7IU6U6zINgE","executionInfo":{"status":"ok","timestamp":1620207262505,"user_tz":-330,"elapsed":48071,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}},"outputId":"9df0dc08-8c4b-4673-db83-2e5d04fe5ad5"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"P-A3R5Z_Jr-B"},"source":["**Data Preprocessing**\n","\n","For data preprocessing, we will do following steps:\n","\n","1. Do not process sentences whose length is less than the MAX_LENGTH\n","2. Do not process any null pair\n","3. Converting into lowercase characters\n","4. Removing all punctutations from the sentences.\n","5. Removing the sentences who have some \"english\" words in hindi sentences.\n","6. Change the encoding of the sentences.\n","7. Build vocabulary for hindi and english languages to map index to unique words and vice versa.\n","8. Converting sentence into tensors for further processing"]},{"cell_type":"code","metadata":{"id":"yP08NGsyJq41","executionInfo":{"status":"ok","timestamp":1620207262506,"user_tz":-330,"elapsed":48065,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["MAX_LENGTH=10\n","\n","class Vocab_builder:\n","    def __init__(self):\n","        self.word_2_index={\"<SOS>\":0,\"<EOS>\":1,\"<PAD>\":2,\"<UKN>\":3}\n","        self.index_2_word={0:\"<SOS>\", 1:\"<EOS>\", 2:\"<PAD>\", 3:\"<UKN>\"}\n","        self.freq={}\n","        self.size=4\n","\n","    def add_this_sentence(self,sentence):\n","        words=sentence.split(\" \")\n","        for word in words:\n","            if word not in self.word_2_index:\n","                #If the word is not there, add it to a new index and store the indexes\n","                #Initialize the frequency of the word to 1 and increase the size of the vocabulary    \n","                self.word_2_index[word]=self.size\n","                self.freq[word]=1     \n","                self.index_2_word[self.size]=word\n","                self.size+=1\n","            else:\n","                # If the word is already present then just increase the frequency\n","                self.freq[word]+=1    "],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"pu1W9XJWMCzv","executionInfo":{"status":"ok","timestamp":1620207262507,"user_tz":-330,"elapsed":48060,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["#Initilizing the objects of hindi and english vocabularies:\n","hindi_vocab=Vocab_builder()\n","eng_vocab=Vocab_builder()"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"0KfIQgM8MRGK","executionInfo":{"status":"ok","timestamp":1620207262508,"user_tz":-330,"elapsed":48055,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["def length(sentence):\n","    '''\n","        Function to tell the length of a sentence.\n","    '''\n","    return len(sentence.split(\" \"))\n","\n","def is_mixed(sentence):\n","    '''\n","        This function will return True if a hindi sentence is containing some english character.\n","    '''\n","    letters=\"abcdefghijklmnopqrstuvwxyz\"\n","    for ch in letters:\n","        if ch in sentence:\n","            return True\n","    return False\n","\n","def preprocess(sentence):\n","    '''\n","        This function will apply the neccesary preprocessing to a sentence\n","    '''\n","    #First we will remove all punctuations from the sentence\n","    punctuations=list(string.punctuation)\n","    cleaned=\"\"\n","    for letter in sentence:\n","        if letter not in punctuations:\n","            cleaned+=letter\n","    cleaned=cleaned.lower() ## Converting into lowercase\n","    return cleaned"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"TtBpfbg8IbkE","executionInfo":{"status":"ok","timestamp":1620207262510,"user_tz":-330,"elapsed":48052,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["def clean_the_data(path):\n","    '''\n","      This function will load the data and process it line by line. \n","      It will apply all the preprocessing and make the data ready for further processing.  \n","    '''\n","    pairs=[]\n","    with open(path,'rt') as f:\n","        data=csv.reader(f, delimiter=',')\n","        row_num=0\n","        for row in data:\n","            if row_num!=0:  #We will not process first row as it will contain header\n","                hindi=row[1]\n","                eng=row[2]\n","                \n","                if length(hindi)>=MAX_LENGTH or length(eng)>=MAX_LENGTH:  #skipping if length is more than MAX_LENGTH\n","                    continue\n","                if not hindi or not eng:  #skipping pair having any NULL value\n","                    continue\n","                if is_mixed(hindi):   #skipping sentence if it contains some english word\n","                    continue\n","                hindi=hindi.encode('utf-8',errors='ignore').decode('utf-8')\n","                eng=eng.encode('ascii',errors='ignore').decode('utf-8')\n","                hindi=preprocess(hindi)\n","                eng=preprocess(eng)\n","                #Adding <SOS>, <EOS> and padding tokens\n","                pair=[hindi.strip(), eng.strip()]\n","\n","                hin_extra=MAX_LENGTH-len(hindi.strip().split(\" \"))\n","                eng_extra=MAX_LENGTH-len(eng.strip().split(\" \"))\n","\n","                hindi_vocab.add_this_sentence(pair[0])\n","                eng_vocab.add_this_sentence(pair[1])\n","                pair[0]=pair[0].split(\" \")\n","                pair[0].insert(0,\"<SOS>\")\n","                pair[0].append(\"<EOS>\")\n","                pair[0]=pair[0]+[\"<PAD>\"]*(hin_extra)\n","\n","                pair[1]=pair[1].split(\" \")\n","                pair[1].insert(0,\"<SOS>\")\n","                pair[1].append(\"<EOS>\")\n","                pair[1]=pair[1]+[\"<PAD>\"]*(eng_extra)\n","\n","                pair[0]=\" \".join(pair[0])\n","                pair[1]=\" \".join(pair[1])\n","                pairs.append(pair)\n","            row_num+=1\n","    return pairs"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"If_qHw3iI0Zj","executionInfo":{"status":"ok","timestamp":1620207262512,"user_tz":-330,"elapsed":48049,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["file_path=\"/content/drive/MyDrive/Colab Notebooks/\"       # path of train dataset in the drive\n","train_file_path=file_path+\"train.csv\""],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"kQD3qXWeYF93","executionInfo":{"status":"ok","timestamp":1620207264258,"user_tz":-330,"elapsed":49790,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["pairs=clean_the_data(train_file_path)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VfjYSISX7LPD","executionInfo":{"status":"ok","timestamp":1620208969136,"user_tz":-330,"elapsed":1582,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}},"outputId":"27d28296-cc4c-48f3-a138-3f74da50cfcd"},"source":["pairs[10]"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<SOS> लानत है तुम पर <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>',\n"," '<SOS> damn you <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>']"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"Gxzf6b1uYRLH","executionInfo":{"status":"ok","timestamp":1620207264270,"user_tz":-330,"elapsed":49800,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["#Now we need to convert each of this pair into corresponding tensors\n","def pair_to_tensor(pair):\n","    '''\n","    A function to convert a given pair to tensors corresponding to index in vocabulary\n","    '''\n","    hindi_sentence=pair[0]\n","    eng_sentence=pair[1]\n","    indexes_hindi=[hindi_vocab.word_2_index[word] for word in hindi_sentence.split(' ')]\n","    indexes_eng=[eng_vocab.word_2_index[word] for word in eng_sentence.split(' ')]\n","    hindi_tensor=torch.tensor(indexes_hindi, dtype=torch.long, device=device).view(-1,1)\n","    eng_tensor=torch.tensor(indexes_eng, dtype=torch.long, device=device).view(-1,1)\n","    return (hindi_tensor, eng_tensor)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"E6erfJEZYVFv","executionInfo":{"status":"ok","timestamp":1620207278493,"user_tz":-330,"elapsed":64017,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["hin_tensors=[]\n","eng_tensors=[]\n","for pair in pairs:      # we will convert each pair into tensor to process it\n","    hin,eng=pair_to_tensor(pair)\n","    hin_tensors.append(hin)\n","    eng_tensors.append(eng)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VqFysuNLVXDr"},"source":["**Transformers**\n","\n","References:\n","1. https://arxiv.org/abs/1706.03762\n","2. https://www.youtube.com/watch?v=iDulhoQ2pro\n","3. https://www.youtube.com/watch?v=TQQlZhbC5ps&t=636s\n","4. https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html"]},{"cell_type":"code","metadata":{"id":"2OPzyQrDVUlZ","executionInfo":{"status":"ok","timestamp":1620207278495,"user_tz":-330,"elapsed":64013,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["class Transformer_model(nn.Module):\n","    def __init__(self, embed_size, len_hin_vocab, len_eng_vocab, src_pad_index, num_heads, enc_layers, dec_layers, forward_exp, dropout, max_length,device):\n","        super(Transformer_model,self).__init__()\n","        self.hin_word_embed=nn.Embedding(len_hin_vocab, embed_size) #shape: (len_hin_vocab, embed_size)\n","        self.eng_word_embed=nn.Embedding(len_eng_vocab, embed_size) #shape: (len_eng_vocab, embed_size)\n","        self.hin_positional_embed=nn.Embedding(max_length, embed_size) #shape: (MAX_LENGTH, embed_size)\n","        self.eng_positional_embed=nn.Embedding(max_length, embed_size)  #shape: (MAX_LENGTH, embed_size)\n","        self.device=device\n","        self.transformer_layer=nn.Transformer(embed_size, num_heads,enc_layers, dec_layers, forward_expansion, dropout) \n","        self.out_fc=nn.Linear(embed_size, len_eng_vocab)    #linear layer to predicted the output word\n","        self.dropout=nn.Dropout(dropout)\n","        self.src_pad_index=src_pad_index\n","\n","    def gen_mask_for_hindi(self, source):\n","        #need to transpose source as padding need to be of size (batch_size, seq_len) but source is of shape (seq_len, batch_size)\n","        source=source.transpose(0,1)  \n","        mask=(source==self.src_pad_index) #(mask will contain 1 where there is pad token, and 0 otherwise)\n","        return mask.to(self.device)\n","\n","    def forward(self, src, target):\n","        hin_seq_length, batch_size=src.shape\n","        eng_seq_length, batch_size=target.shape\n","        # creating positional embeddings to encode position of words in transformer (it will be just a range array upto max_length)\n","        hin_positional=torch.arange(0,hin_seq_length).unsqueeze(1).expand(hin_seq_length, batch_size).to(self.device)\n","        eng_positional=torch.arange(0,eng_seq_length).unsqueeze(1).expand(eng_seq_length, batch_size).to(self.device)\n","        # calculating embeddings as sum of positional and word embeddings\n","        hin_embedding=self.dropout(self.hin_word_embed(src)+self.hin_positional_embed(hin_positional))\n","        eng_embedding=self.dropout(self.eng_word_embed(target)+self.eng_positional_embed(eng_positional))\n","        # generating padding mask for hindi (source)\n","        hindi_padding_mask=self.gen_mask_for_hindi(src)\n","        # using in-built transformer function to generate mask for english (target)\n","        # It will be in form of a lower-triangular matrix\n","        eng_mask=self.transformer_layer.generate_square_subsequent_mask(eng_seq_length).to(self.device)\n","        output=self.transformer_layer(hin_embedding, eng_embedding, src_key_padding_mask=hindi_padding_mask, tgt_mask=eng_mask)\n","        output=self.out_fc(output)\n","        return output"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yh0WtJgzeK1x"},"source":["**Training the Model**"]},{"cell_type":"code","metadata":{"id":"VcOY4wESkGpG","executionInfo":{"status":"ok","timestamp":1620207278497,"user_tz":-330,"elapsed":64010,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["# defining model parameters\n","embed_size=512\n","len_hin_vocab=hindi_vocab.size\n","len_eng_vocab=eng_vocab.size\n","padding_idx=eng_vocab.word_2_index[\"<PAD>\"]\n","num_heads=8\n","enc_layers, dec_layers= 1,1\n","dropout=0.10\n","forward_expansion=4"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"lz0azI1NkDoI","executionInfo":{"status":"ok","timestamp":1620207278499,"user_tz":-330,"elapsed":64004,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["model=Transformer_model(embed_size, len_hin_vocab, len_eng_vocab, padding_idx, num_heads, enc_layers, dec_layers, forward_expansion, dropout, MAX_LENGTH+2,device).to(device)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"DsQ7lB6dsPgf","executionInfo":{"status":"ok","timestamp":1620207278501,"user_tz":-330,"elapsed":64001,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["model_available=False # A variable to indicate whether a model is present in the path or not"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"QRrEbs8HeIXo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620208316850,"user_tz":-330,"elapsed":1102344,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}},"outputId":"c97b4f53-183d-4256-cdb6-aef8c09c7c5c"},"source":["batch_size=64\n","optimizer=optim.Adam(model.parameters(),lr=0.001)  \n","PATH=\"/content/drive/MyDrive/Colab Notebooks/test_phase/final_phase_v1_10_demo.pth\"\n","\n","epochs=20 \n","epoch_loss=0.0\n","\n","criterion=nn.CrossEntropyLoss(ignore_index=padding_idx) #ignore padding index while calculating loss\n","\n","train_model=True #if need to train the model again, set it to True\n","\n","if train_model==False:\n","    model=torch.load(PATH)\n","else:\n","    if model_available:\n","        model=torch.load(PATH)\n","    batches=len(pairs)//batch_size\n","    for epoch in range(epochs):\n","        print(f\"epoch {epoch+1}/{epochs}\")\n","        model.eval()\n","        model.train(True)\n","        cur_batch=0\n","        for idx in range(0,len(pairs),batch_size):\n","            # will do processing for each batch\n","            cur_batch+=1\n","            if(cur_batch%100==0):\n","                print(f\"    running batch {cur_batch} of {batches}\")\n","            if idx+batch_size < len(pairs):\n","                src_batch=hin_tensors[idx:idx+batch_size]\n","                target_batch=eng_tensors[idx:idx+batch_size]\n","            else:\n","                src_batch=hin_tensors[idx:]\n","                target_batch=eng_tensors[idx:]\n","\n","            src_batch=torch.cat(src_batch,dim=1).to(device)    #shape: (max_lenbatch_size)\n","            target_batch=torch.cat(target_batch,dim=1).to(device) #shape: (max_lenbatch_size)\n","            output=model(src_batch,target_batch[:-1,:])\n","            output=output.reshape(-1, output.shape[2])\n","\n","            target=target_batch[1:].reshape(-1)\n","\n","            optimizer.zero_grad()\n","            loss=criterion(output,target)\n","\n","            loss.backward()\n","            # restrict gradients from exploding\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","        \n","        print(f\"Epoch loss : {loss.item()}\")\n","        \n","        torch.save(model,PATH)\n","        model_available=True"],"execution_count":15,"outputs":[{"output_type":"stream","text":["epoch 1/20\n","    running batch 100 of 861\n","    running batch 200 of 861\n","    running batch 300 of 861\n","    running batch 400 of 861\n","    running batch 500 of 861\n","    running batch 600 of 861\n","    running batch 700 of 861\n","    running batch 800 of 861\n","Epoch loss : 3.949277877807617\n","epoch 2/20\n","    running batch 100 of 861\n","    running batch 200 of 861\n","    running batch 300 of 861\n","    running batch 400 of 861\n","    running batch 500 of 861\n","    running batch 600 of 861\n","    running batch 700 of 861\n","    running batch 800 of 861\n","Epoch loss : 3.4154295921325684\n","epoch 3/20\n","    running batch 100 of 861\n","    running batch 200 of 861\n","    running batch 300 of 861\n","    running batch 400 of 861\n","    running batch 500 of 861\n","    running batch 600 of 861\n","    running batch 700 of 861\n","    running batch 800 of 861\n","Epoch loss : 3.1742632389068604\n","epoch 4/20\n","    running batch 100 of 861\n","    running batch 200 of 861\n","    running batch 300 of 861\n","    running batch 400 of 861\n","    running batch 500 of 861\n","    running batch 600 of 861\n","    running batch 700 of 861\n","    running batch 800 of 861\n","Epoch loss : 2.7879676818847656\n","epoch 5/20\n","    running batch 100 of 861\n","    running batch 200 of 861\n","    running batch 300 of 861\n","    running batch 400 of 861\n","    running batch 500 of 861\n","    running batch 600 of 861\n","    running batch 700 of 861\n","    running batch 800 of 861\n","Epoch loss : 2.6715281009674072\n","epoch 6/20\n","    running batch 100 of 861\n","    running batch 200 of 861\n","    running batch 300 of 861\n","    running batch 400 of 861\n","    running batch 500 of 861\n","    running batch 600 of 861\n","    running batch 700 of 861\n","    running batch 800 of 861\n","Epoch loss : 2.520691394805908\n","epoch 7/20\n","    running batch 100 of 861\n","    running batch 200 of 861\n","    running batch 300 of 861\n","    running batch 400 of 861\n","    running batch 500 of 861\n","    running batch 600 of 861\n","    running batch 700 of 861\n","    running batch 800 of 861\n","Epoch loss : 2.1732850074768066\n","epoch 8/20\n","    running batch 100 of 861\n","    running batch 200 of 861\n","    running batch 300 of 861\n","    running batch 400 of 861\n","    running batch 500 of 861\n","    running batch 600 of 861\n","    running batch 700 of 861\n","    running batch 800 of 861\n","Epoch loss : 2.0884881019592285\n","epoch 9/20\n","    running batch 100 of 861\n","    running batch 200 of 861\n","    running batch 300 of 861\n","    running batch 400 of 861\n","    running batch 500 of 861\n","    running batch 600 of 861\n","    running batch 700 of 861\n","    running batch 800 of 861\n","Epoch loss : 1.982195258140564\n","epoch 10/20\n","    running batch 100 of 861\n","    running batch 200 of 861\n","    running batch 300 of 861\n","    running batch 400 of 861\n","    running batch 500 of 861\n","    running batch 600 of 861\n","    running batch 700 of 861\n","    running batch 800 of 861\n","Epoch loss : 2.0496580600738525\n","epoch 11/20\n","    running batch 100 of 861\n","    running batch 200 of 861\n","    running batch 300 of 861\n","    running batch 400 of 861\n","    running batch 500 of 861\n","    running batch 600 of 861\n","    running batch 700 of 861\n","    running batch 800 of 861\n","Epoch loss : 1.916334629058838\n","epoch 12/20\n","    running batch 100 of 861\n","    running batch 200 of 861\n","    running batch 300 of 861\n","    running batch 400 of 861\n","    running batch 500 of 861\n","    running batch 600 of 861\n","    running batch 700 of 861\n","    running batch 800 of 861\n","Epoch loss : 1.8844746351242065\n","epoch 13/20\n","    running batch 100 of 861\n","    running batch 200 of 861\n","    running batch 300 of 861\n","    running batch 400 of 861\n","    running batch 500 of 861\n","    running batch 600 of 861\n","    running batch 700 of 861\n","    running batch 800 of 861\n","Epoch loss : 1.828260898590088\n","epoch 14/20\n","    running batch 100 of 861\n","    running batch 200 of 861\n","    running batch 300 of 861\n","    running batch 400 of 861\n","    running batch 500 of 861\n","    running batch 600 of 861\n","    running batch 700 of 861\n","    running batch 800 of 861\n","Epoch loss : 1.6661972999572754\n","epoch 15/20\n","    running batch 100 of 861\n","    running batch 200 of 861\n","    running batch 300 of 861\n","    running batch 400 of 861\n","    running batch 500 of 861\n","    running batch 600 of 861\n","    running batch 700 of 861\n","    running batch 800 of 861\n","Epoch loss : 1.6440019607543945\n","epoch 16/20\n","    running batch 100 of 861\n","    running batch 200 of 861\n","    running batch 300 of 861\n","    running batch 400 of 861\n","    running batch 500 of 861\n","    running batch 600 of 861\n","    running batch 700 of 861\n","    running batch 800 of 861\n","Epoch loss : 1.4807807207107544\n","epoch 17/20\n","    running batch 100 of 861\n","    running batch 200 of 861\n","    running batch 300 of 861\n","    running batch 400 of 861\n","    running batch 500 of 861\n","    running batch 600 of 861\n","    running batch 700 of 861\n","    running batch 800 of 861\n","Epoch loss : 1.3757643699645996\n","epoch 18/20\n","    running batch 100 of 861\n","    running batch 200 of 861\n","    running batch 300 of 861\n","    running batch 400 of 861\n","    running batch 500 of 861\n","    running batch 600 of 861\n","    running batch 700 of 861\n","    running batch 800 of 861\n","Epoch loss : 1.2908620834350586\n","epoch 19/20\n","    running batch 100 of 861\n","    running batch 200 of 861\n","    running batch 300 of 861\n","    running batch 400 of 861\n","    running batch 500 of 861\n","    running batch 600 of 861\n","    running batch 700 of 861\n","    running batch 800 of 861\n","Epoch loss : 1.2434991598129272\n","epoch 20/20\n","    running batch 100 of 861\n","    running batch 200 of 861\n","    running batch 300 of 861\n","    running batch 400 of 861\n","    running batch 500 of 861\n","    running batch 600 of 861\n","    running batch 700 of 861\n","    running batch 800 of 861\n","Epoch loss : 1.1340436935424805\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"K44QRGgOfMQH","executionInfo":{"status":"ok","timestamp":1620208316855,"user_tz":-330,"elapsed":1102344,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["def clean_sentence(sentence):\n","    '''\n","      Function to remove the punctuation from the test sentence\n","    '''\n","    punctuations=list(string.punctuation)\n","    cleaned=\"\"\n","    for letter in sentence:\n","        if letter=='<' or letter=='>' or letter not in punctuations:\n","            cleaned+=letter\n","    return cleaned  \n","\n","def predict_translation(model,sentence,device,max_length=MAX_LENGTH):\n","    '''\n","      function will return the translation predicted by the trained model for each sentence\n","    '''\n","    sentence=clean_sentence(sentence)\n","    tokens=sentence.split(\" \")\n","    indexes=[]\n","    for token in tokens:\n","        if token in hindi_vocab.word_2_index:\n","            indexes.append(hindi_vocab.word_2_index[token])\n","        else:\n","            indexes.append(hindi_vocab.word_2_index[\"<UKN>\"])\n","    indexes=indexes[:MAX_LENGTH+2]  # model is trained on MAX_LENGTH sentences only so it expects sentences of this length only \n","    tensor_of_sentence=torch.LongTensor(indexes).unsqueeze(1).to(device)\n","    outputs=[0]   #adding <SOS> in the beginning of output\n","    for _ in range(max_length):\n","        target_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n","        with torch.no_grad():\n","            output=model(tensor_of_sentence,target_tensor)\n","        pred=output.argmax(2)[-1, :].item()\n","\n","        outputs.append(pred)\n","\n","        if eng_vocab.index_2_word[pred] ==\"<EOS>\":\n","            break\n","    \n","    final=[]\n","\n","    for i in outputs:\n","        if i == \"<PAD>\":\n","            break\n","        final.append(i)\n","\n","    final = [eng_vocab.index_2_word[idx] for idx in final]\n","    translated=\" \".join(final)\n","    return translated"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"st7xPjpRfvPh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620208318333,"user_tz":-330,"elapsed":1103811,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}},"outputId":"abb420a5-a9eb-4309-f764-f351436ed812"},"source":["# Checking how the model is translating sentences from the train set\n","test_sentences=[pair[0] for pair in pairs[125:150]]\n","actual_sentences=[pair[1] for pair in pairs[125:150]]\n","pred_sentences=[]\n","\n","for idx,i in enumerate(test_sentences):\n","    translated=predict_translation(model,i,device)\n","    print(\"*\"*20)\n","    print(f\"Hindi: {i}\")\n","    print(f\"Actual: {actual_sentences[idx]}\")\n","    print(f\"Predicted: {translated}\")\n","    print(\"*\"*20)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["********************\n","Hindi: <SOS> दीदी फ़ूल दे दीजिये <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Actual: <SOS> sister flower please <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Predicted: <SOS> sister flower please <EOS>\n","********************\n","********************\n","Hindi: <SOS> ग्रन्ट्स <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Actual: <SOS> grunts <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Predicted: <SOS> grunts <EOS>\n","********************\n","********************\n","Hindi: <SOS> ca चलिए उसे यहाँ दिखाते हैं <EOS> <PAD> <PAD> <PAD> <PAD>\n","Actual: <SOS> ca i think we have that lets show that <EOS> <PAD>\n","Predicted: <SOS> ca and show here how are <EOS>\n","********************\n","********************\n","Hindi: <SOS> नुक्सान किन में जमा हो सकता है <EOS> <PAD> <PAD> <PAD>\n","Actual: <SOS> what can damage accumulate in <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Predicted: <SOS> can damage accumulate in there <EOS>\n","********************\n","********************\n","Hindi: <SOS> और मैं यहीं रुकता हूं धन्यवाद <EOS> <PAD> <PAD> <PAD> <PAD>\n","Actual: <SOS> thank you <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Predicted: <SOS> and i thank you thanks <EOS>\n","********************\n","********************\n","Hindi: <SOS> कौनसी नीतियां <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Actual: <SOS> what policies <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Predicted: <SOS> what policies <EOS>\n","********************\n","********************\n","Hindi: <SOS> मेरा गाना हो गया। <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Actual: <SOS> im done <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Predicted: <SOS> my song is being <EOS>\n","********************\n","********************\n","Hindi: <SOS> वो ये क्यों कर रहा है <EOS> <PAD> <PAD> <PAD> <PAD>\n","Actual: <SOS> why is he doing this <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Predicted: <SOS> why is he doing this <EOS>\n","********************\n","********************\n","Hindi: <SOS> अब आप अपने सवाल पूछते हो। <EOS> <PAD> <PAD> <PAD> <PAD>\n","Actual: <SOS> now ask your questions <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Predicted: <SOS> now you answer yourself <EOS>\n","********************\n","********************\n","Hindi: <SOS> चलो। <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Actual: <SOS> go go <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Predicted: <SOS> come on <EOS>\n","********************\n","********************\n","Hindi: <SOS> मैं कभी आत्मसमर्पण नहीं करूंगा <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Actual: <SOS> i will never surrender <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Predicted: <SOS> i never surrender <EOS>\n","********************\n","********************\n","Hindi: <SOS> वरना यहाँ खड़े नहीं होते <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Actual: <SOS> were all immune or we wouldnt still be here <EOS> <PAD>\n","Predicted: <SOS> otherwise they will not be here <EOS>\n","********************\n","********************\n","Hindi: <SOS> क्या उनकी कोई छुपी हुई योजना है <EOS> <PAD> <PAD> <PAD>\n","Actual: <SOS> do they have a hidden agenda <EOS> <PAD> <PAD> <PAD> <PAD>\n","Predicted: <SOS> do not have a hidden agenda <EOS>\n","********************\n","********************\n","Hindi: <SOS> मैं तुम्हें बचाने के लिए भागी। <EOS> <PAD> <PAD> <PAD> <PAD>\n","Actual: <SOS> i rushed to protect you <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Predicted: <SOS> i rushed to protect you <EOS>\n","********************\n","********************\n","Hindi: <SOS> मैं इसे शांति का सूत्र कहता हूँ। <EOS> <PAD> <PAD> <PAD>\n","Actual: <SOS> i call this my peace formula <EOS> <PAD> <PAD> <PAD> <PAD>\n","Predicted: <SOS> i call it with formula <EOS>\n","********************\n","********************\n","Hindi: <SOS> आदमी फोन पर <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Actual: <SOS> man on phone <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Predicted: <SOS> man call on man <EOS>\n","********************\n","********************\n","Hindi: <SOS> यह रेंजर बेस आपातकालीन है <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Actual: <SOS> this is ranger base emergency <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Predicted: <SOS> this is ranger base emergency <EOS>\n","********************\n","********************\n","Hindi: <SOS> तुम्हें मिल गया <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Actual: <SOS> you got it <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Predicted: <SOS> you got it <EOS>\n","********************\n","********************\n","Hindi: <SOS> अपने सभी नए धन आप वजन नीचे <EOS> <PAD> <PAD> <PAD>\n","Actual: <SOS> all your new riches weighing you down <EOS> <PAD> <PAD> <PAD>\n","Predicted: <SOS> your new quarters you read <EOS>\n","********************\n","********************\n","Hindi: <SOS> मैं आप की सराहना कर सकता हूँ <EOS> <PAD> <PAD> <PAD>\n","Actual: <SOS> im sure you can appreciate <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Predicted: <SOS> i can appreciate you <EOS>\n","********************\n","********************\n","Hindi: <SOS> आप्का सर <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Actual: <SOS> your head <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Predicted: <SOS> your head upstairs <EOS>\n","********************\n","********************\n","Hindi: <SOS> भगवान ने मुझे इस हार नहीं मानी। <EOS> <PAD> <PAD> <PAD>\n","Actual: <SOS> god didnt give me this <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Predicted: <SOS> god no offense <EOS>\n","********************\n","********************\n","Hindi: <SOS> इसने कहा तुम निकम्मे हो। <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Actual: <SOS> he said youre a loser <EOS> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Predicted: <SOS> he said you were looking for a <EOS>\n","********************\n","********************\n","Hindi: <SOS> मुझे माफ करना मेरे प्यार कर रहा हूँ। <EOS> <PAD> <PAD>\n","Actual: <SOS> i am so sorry my love <EOS> <PAD> <PAD> <PAD> <PAD>\n","Predicted: <SOS> i am so sorry love my voice <EOS>\n","********************\n","********************\n","Hindi: <SOS> हर कोई प्यार के लिए चारों ओर चला जाएगा <EOS> <PAD>\n","Actual: <SOS> everyone will run around for love <EOS> <PAD> <PAD> <PAD> <PAD>\n","Predicted: <SOS> everyone will be around for love for everyone <EOS>\n","********************\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SCItkDu9pfiv","executionInfo":{"status":"ok","timestamp":1620208318336,"user_tz":-330,"elapsed":1103809,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["# Opening the file to write the translations\n","fp=open(\"/content/drive/MyDrive/Colab Notebooks/test_phase/answer.txt\",\"w\")"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"YNWk_8ynnPAA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620208967526,"user_tz":-330,"elapsed":1752985,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}},"outputId":"b720da2a-a9ef-4f2f-b782-568f0db37b68"},"source":["# Path of the validation/test data \n","val_data_path=\"/content/drive/MyDrive/Colab Notebooks/testhindistatements.csv\"\n","with open(val_data_path, 'rt') as f:\n","    data=csv.reader(f, delimiter=',')\n","    row_num=0\n","    for row in data:\n","        if row_num==0:\n","            row_num+=1\n","            continue\n","        sentence=row[2].strip()\n","        translated=predict_translation(model,sentence,device)\n","        translated=translated.split(\" \")[1:-1]  #removing SOS and EOS token before writing to the file \n","        translated=\" \".join(translated)\n","        fp.write(translated+'\\n')\n","        if row_num%500==0:\n","            print(f\"sentence : {row_num}\")\n","        row_num+=1\n","fp.close()\n","        "],"execution_count":19,"outputs":[{"output_type":"stream","text":["sentence : 500\n","sentence : 1000\n","sentence : 1500\n","sentence : 2000\n","sentence : 2500\n","sentence : 3000\n","sentence : 3500\n","sentence : 4000\n","sentence : 4500\n","sentence : 5000\n","sentence : 5500\n","sentence : 6000\n","sentence : 6500\n","sentence : 7000\n","sentence : 7500\n","sentence : 8000\n","sentence : 8500\n","sentence : 9000\n","sentence : 9500\n","sentence : 10000\n","sentence : 10500\n","sentence : 11000\n","sentence : 11500\n","sentence : 12000\n","sentence : 12500\n","sentence : 13000\n","sentence : 13500\n","sentence : 14000\n","sentence : 14500\n","sentence : 15000\n","sentence : 15500\n","sentence : 16000\n","sentence : 16500\n","sentence : 17000\n","sentence : 17500\n","sentence : 18000\n","sentence : 18500\n","sentence : 19000\n","sentence : 19500\n","sentence : 20000\n","sentence : 20500\n","sentence : 21000\n","sentence : 21500\n","sentence : 22000\n","sentence : 22500\n","sentence : 23000\n","sentence : 23500\n","sentence : 24000\n"],"name":"stdout"}]}]}