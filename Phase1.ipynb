{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Phase1.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"G0wL7_HbsJqt"},"source":["# Data loading and cleaning"]},{"cell_type":"markdown","metadata":{"id":"VZ01zjtH_GjL"},"source":["**Note:** To run the notebook upload given \"train.csv\" and \"hindistatements_week1.csv\" for phase 1 in the drive inside collaboratory folder."]},{"cell_type":"code","metadata":{"id":"lENiIDU6yGeT","executionInfo":{"status":"ok","timestamp":1619499570545,"user_tz":-330,"elapsed":1821,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["import string\n","import numpy as np\n","import pandas as pd\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WA588G4Z0JVh"},"source":["We will read the data from drive directly. For safety we will make a copy of the data and drop the first column from the data."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mv02B9QdEDs1","executionInfo":{"status":"ok","timestamp":1619499572384,"user_tz":-330,"elapsed":3651,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}},"outputId":"90028036-0d21-4539-a91b-695077de4627"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gyMEvHjeyg6J","executionInfo":{"status":"ok","timestamp":1619499572385,"user_tz":-330,"elapsed":3644,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["data=pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/train.csv\")\n","data_copy=data.copy()\n","data.drop(data.columns[0],axis=1,inplace=True)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"guJcm_Mc0cml"},"source":["**Creating columns for sentence length for both Hindi and English**\n","\n","Sentence length can play important role in cleaning the data, so we can create two new columns for lengths of hindi and english sentences."]},{"cell_type":"code","metadata":{"id":"kS6jMjc2zL7c","executionInfo":{"status":"ok","timestamp":1619499572940,"user_tz":-330,"elapsed":4193,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["data['len_hindi']=data['hindi'].apply(lambda x:len(x.split(' ')))\n","data['len_english']=data['english'].apply(lambda x: len(x.split(' ')))"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VSvCyhRM0vJd"},"source":["**Checking for null values**\n","\n","checking for null values is generally the first step in the data analaysis. Here also we can check if any null values are present in the given dataset or not."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rUfTSK9ezR2l","executionInfo":{"status":"ok","timestamp":1619499572941,"user_tz":-330,"elapsed":4188,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}},"outputId":"ee374dab-55da-4995-a2a0-618437301d94"},"source":["data.isnull().sum()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["hindi          0\n","english        0\n","len_hindi      0\n","len_english    0\n","dtype: int64"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"ER9WKBdj01-o"},"source":["There is no null values present in the data"]},{"cell_type":"markdown","metadata":{"id":"kR6FBmjR05u4"},"source":["**Dropping duplicates from the data**\n","\n","It may be the case that there are some duplicate entries present in the dataset. We can remove such entries."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pznQ-9JbzTC9","executionInfo":{"status":"ok","timestamp":1619499572941,"user_tz":-330,"elapsed":4178,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}},"outputId":"732148df-b88e-4b48-bf1b-b8e7d8a20b86"},"source":["print(f\"Shape of the dataset before removing the duplicates: {data.shape}\")\n","data.drop_duplicates(inplace=True)\n","print(f\"Shape of the dataset after removing the duplicates: {data.shape}\")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Shape of the dataset before removing the duplicates: (102322, 4)\n","Shape of the dataset after removing the duplicates: (102296, 4)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Bb29BLPG0_9e"},"source":["We can see there were some duplicates present in the dataset."]},{"cell_type":"markdown","metadata":{"id":"NpFEQ3nO1EhJ"},"source":["**Converting all chars in the lowercase**\n","\n","We can perform lowercase normalization on the whole data."]},{"cell_type":"code","metadata":{"id":"t023QkIgzWFd","executionInfo":{"status":"ok","timestamp":1619499572942,"user_tz":-330,"elapsed":4171,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["data['hindi']=data['hindi'].apply(lambda x: x.lower())\n","data['english']=data['english'].apply(lambda x: x.lower())"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l1v8dNsJ1MPo"},"source":["**Removing punctuations**\n","\n","There are different kinds of punctuations present in both hindi and english sentences. It's better to remove such punctuations in the data cleaning.\n","\n"]},{"cell_type":"code","metadata":{"id":"NNjD0cprza0e","executionInfo":{"status":"ok","timestamp":1619499572943,"user_tz":-330,"elapsed":4166,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["def remove_punctuations(sentence):\n","    punctuations=list(string.punctuation)\n","    cleaned=\"\"\n","    for letter in sentence:\n","        if letter not in punctuations:\n","            cleaned+=letter\n","    return cleaned  "],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"O1HJSO0YzbeM","executionInfo":{"status":"ok","timestamp":1619499580939,"user_tz":-330,"elapsed":12155,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["data['hindi']=data['hindi'].apply(lambda x: remove_punctuations(x))\n","data['english']=data['english'].apply(lambda x: remove_punctuations(x))"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RRcdvyvi1Sr5"},"source":["**Removing mixed sentences (those samples which have english words in the hindi sentences)**\n","\n","On observing the data, we find out that there are some samples in which english words are present between the hindi sentences. We treat these sentences as outliers and can remove them from the dataset."]},{"cell_type":"code","metadata":{"id":"9COkSIBIzetd","executionInfo":{"status":"ok","timestamp":1619499580947,"user_tz":-330,"elapsed":12157,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["def is_mixed(sentence):\n","    letters=\"abcdefghijklmnopqrstuvwxyz\"\n","    for ch in letters:\n","        if ch in sentence:\n","            return True\n","    return False"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"z4yjQfefzheF","executionInfo":{"status":"ok","timestamp":1619499580949,"user_tz":-330,"elapsed":12151,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}},"outputId":"b5037289-0788-4e1b-d4ff-125f7525b7d8"},"source":["data['is_mixed']=data['hindi'].apply(lambda x : is_mixed(x))\n","data.head()"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>hindi</th>\n","      <th>english</th>\n","      <th>len_hindi</th>\n","      <th>len_english</th>\n","      <th>is_mixed</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>एल सालवाडोर मे जिन दोनो पक्षों ने सिविलयुद्ध स...</td>\n","      <td>in el salvador both sides that withdrew from t...</td>\n","      <td>22</td>\n","      <td>23</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>मैं उनके साथ कोई लेना देना नहीं है</td>\n","      <td>i have nothing to do with them</td>\n","      <td>8</td>\n","      <td>7</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>हटाओ रिक</td>\n","      <td>fuck them rick</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>क्योंकि यह एक खुशियों भरी फ़िल्म है</td>\n","      <td>because its a happy film</td>\n","      <td>7</td>\n","      <td>5</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>the thought reaching the eyes</td>\n","      <td>the thought reaching the eyes</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>True</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               hindi  ... is_mixed\n","0  एल सालवाडोर मे जिन दोनो पक्षों ने सिविलयुद्ध स...  ...    False\n","1                 मैं उनके साथ कोई लेना देना नहीं है  ...    False\n","2                                           हटाओ रिक  ...    False\n","3                क्योंकि यह एक खुशियों भरी फ़िल्म है  ...    False\n","4                      the thought reaching the eyes  ...     True\n","\n","[5 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8P_O575Nzjfu","executionInfo":{"status":"ok","timestamp":1619499580950,"user_tz":-330,"elapsed":12142,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}},"outputId":"d9b999ed-afa1-4a5d-95bb-f070a8202c74"},"source":["data['is_mixed'].value_counts(normalize=True)*100"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False    94.430867\n","True      5.569133\n","Name: is_mixed, dtype: float64"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"5V4NZGpazmdG","executionInfo":{"status":"ok","timestamp":1619499580951,"user_tz":-330,"elapsed":12134,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["data=data[data['is_mixed']==False]"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FZOCLt7i14VK"},"source":["**Changing the Encoding of the data**\n","\n","The data may have some 'unicode' encoding. We need to change the encoding for processing of the data."]},{"cell_type":"code","metadata":{"id":"2rOK_kh11j_Y","executionInfo":{"status":"ok","timestamp":1619499580952,"user_tz":-330,"elapsed":12129,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["data['hindi']=data['hindi'].str.encode('utf-8',errors='ignore').str.decode('utf-8')\n","data['english']=data['english'].str.encode('ascii',errors='ignore').str.decode('utf-8')"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0q_MjVye4qlF"},"source":["**Dropping any row having NULL values**"]},{"cell_type":"code","metadata":{"id":"ONBkWUc_1lAY","executionInfo":{"status":"ok","timestamp":1619499613714,"user_tz":-330,"elapsed":44885,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["null_indices=[]\n","for index,rows in data.iterrows():\n","    is_null=rows.isnull()\n","    if is_null.any():\n","        null_indices.append(index)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"HHvdyk6u1qnw","executionInfo":{"status":"ok","timestamp":1619499613716,"user_tz":-330,"elapsed":44881,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["data.drop(null_indices,inplace=True)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iehtYng24xDR"},"source":["**Saving the processed Dataframe**\n","\n","We will save this processed dataset into drive, so we don't have to repeat these steps again and again."]},{"cell_type":"code","metadata":{"id":"J21sUbxdzozH","executionInfo":{"status":"ok","timestamp":1619499614520,"user_tz":-330,"elapsed":45679,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["data.to_csv(\"/content/drive/MyDrive/Colab Notebooks/processed.csv\")"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R5Y8KFeK5DSz"},"source":["# Loading processed Dataset and Vocabulary building\n","\n","We will load the processed dataframe directly and build the vocabulary for source and target language.\n","\n","### References:\n","1. https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n","2. https://www.youtube.com/watch?v=B8g-PNT2W2Q\n","3. https://www.youtube.com/watch?v=EoGUlvhRYpk"]},{"cell_type":"code","metadata":{"id":"G-qDywzlzs02","executionInfo":{"status":"ok","timestamp":1619499615139,"user_tz":-330,"elapsed":46292,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["data=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/processed.csv')"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"k18AvxCQ8sU8","executionInfo":{"status":"ok","timestamp":1619499648605,"user_tz":-330,"elapsed":79752,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["null_indices=[]\n","for index,rows in data.iterrows():\n","    is_null=rows.isnull()\n","    if is_null.any():\n","        null_indices.append(index)\n","\n","data.drop(null_indices,inplace=True)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"CWaCnnWT8LC-","executionInfo":{"status":"ok","timestamp":1619499648612,"user_tz":-330,"elapsed":79750,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["SOS_token=0\n","EOS_token=1\n","PAD_token=2\n","MAX_LENGTH=25\n","\n","class Vocab_class:\n","    def __init__(self):\n","        self.word_to_index={\"<SOS>\":0,\"<EOS>\":1,\"<PAD>\":2,\"<UKN>\":3}\n","        self.word_counts={}\n","        self.index_to_word={0:\"<SOS>\", 1:\"<EOS>\", 2:\"<PAD>\", 3:\"<UKN>\"}\n","        self.num_of_words=4\n","        \n","    def sentence_add(self, sentence):\n","        words=sentence.split(\" \")\n","        for word in words:\n","            if word not in self.word_to_index:\n","                self.word_to_index[word]=self.num_of_words\n","                self.word_counts[word]=1\n","                \n","                self.index_to_word[self.num_of_words]=word\n","                self.num_of_words+=1\n","            else:\n","                self.word_counts[word]+=1"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"47UzzWbv8WoO","executionInfo":{"status":"ok","timestamp":1619499664333,"user_tz":-330,"elapsed":95464,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["# We scan each sentence from the dataset and add the words in the corresponding vocabularies.\n","hindi_lang=Vocab_class()\n","eng_lang=Vocab_class()\n","pairs=[]\n","# For the sentences which are shorter than the max_length, we use \"<PAD>\" tokens for them.\n","for index,row in data.iterrows():\n","    if row['len_hindi']<MAX_LENGTH and row['len_english']<MAX_LENGTH:\n","        pair=[row['hindi'].strip(), row['english'].strip()]\n","        hin_extra=MAX_LENGTH-len(row['hindi'].strip().split(\" \"))\n","        eng_extra=MAX_LENGTH-len(row['english'].strip().split(\" \"))\n","        hindi_lang.sentence_add(pair[0])\n","        eng_lang.sentence_add(pair[1])\n","        pair[0]=pair[0].split(\" \")\n","        pair[0].insert(0,\"<SOS>\")\n","        pair[0].append(\"<EOS>\")\n","        pair[0]=pair[0]+[\"<PAD>\"]*(hin_extra)\n","\n","        pair[1]=pair[1].split(\" \")\n","        pair[1].insert(0,\"<SOS>\")\n","        pair[1].append(\"<EOS>\")\n","        pair[1]=pair[1]+[\"<PAD>\"]*(eng_extra)\n","\n","        pair[0]=\" \".join(pair[0])\n","        pair[1]=\" \".join(pair[1])\n","        pairs.append(pair)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oOfNudIC8lLU","executionInfo":{"status":"ok","timestamp":1619499664334,"user_tz":-330,"elapsed":95460,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}},"outputId":"31ab7d04-7059-4722-dbfa-d330ddc7b588"},"source":["print(f\"Hindi vocabulary size : {hindi_lang.num_of_words}\")\n","print(f\"English vocabulary size: {eng_lang.num_of_words}\")"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Hindi vocabulary size : 36160\n","English vocabulary size: 25527\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6JWbw8V49V1F","executionInfo":{"status":"ok","timestamp":1619499664336,"user_tz":-330,"elapsed":95454,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["def pair_to_tensor(pair):\n","    '''\n","    A function to convert a given pair to tensors corresponding to index in vocabulary\n","    '''\n","    hindi_sentence=pair[0]\n","    eng_sentence=pair[1]\n","    indexes_hindi=[hindi_lang.word_to_index[word] for word in hindi_sentence.split(' ')]\n","    indexes_eng=[eng_lang.word_to_index[word] for word in eng_sentence.split(' ')]\n","    hindi_tensor=torch.tensor(indexes_hindi, dtype=torch.long, device=device).view(-1,1)\n","    eng_tensor=torch.tensor(indexes_eng, dtype=torch.long, device=device).view(-1,1)\n","    return (hindi_tensor, eng_tensor)"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"cZdheq4X9bN9","executionInfo":{"status":"ok","timestamp":1619499674107,"user_tz":-330,"elapsed":105219,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["# Now we can convert all the sentences into tensors for further processing.\n","hin_tensors=[]\n","eng_tensors=[]\n","for pair in pairs:\n","    hin,eng=pair_to_tensor(pair)\n","    hin_tensors.append(hin)\n","    eng_tensors.append(eng)"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KXvo9f249qnV"},"source":["# Seq2Seq Model Implementation\n","\n","#### References:\n","1. https://machinelearningmastery.com/encoder-decoder-recurrent-neural-network-models-neural-machine-translation/\n","2. https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346\n","\n","\n","### Encoder RNN Implementation"]},{"cell_type":"code","metadata":{"id":"TTvuAI449c4c","executionInfo":{"status":"ok","timestamp":1619499674109,"user_tz":-330,"elapsed":105216,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["class EncoderLSTM(nn.Module):\n","    def __init__(self,size_input,size_embbeding,size_hidden,layers,p):\n","        super(EncoderLSTM,self).__init__()\n","        self.size_input=size_input\n","        self.size_embbeding=size_embbeding\n","        self.size_hidden=size_hidden\n","        self.layers=layers\n","        self.dropout=nn.Dropout(p)\n","        self.tag=True\n","\n","        self.embbed_layer=nn.Embedding(self.size_input,self.size_embbeding)\n","        self.lstm=nn.LSTM(self.size_embbeding,self.size_hidden,self.layers,dropout=p)\n","\n","    def forward(self, x):\n","        # print(x.shape)\n","        embbeding=self.dropout(self.embbed_layer(x))\n","        # print(embbeding.shape)\n","        output, (hidden_st,cell_st) = self.lstm(embbeding)\n","        return hidden_st, cell_st"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7n6Z543l96gq"},"source":["### Decoder RNN Implementation\n"]},{"cell_type":"code","metadata":{"id":"0foG5GUE99zP","executionInfo":{"status":"ok","timestamp":1619499674110,"user_tz":-330,"elapsed":105209,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["class DecoderLSTM(nn.Module):\n","    def __init__(self,size_input,size_embbeding,size_hidden,layers,p,size_output):\n","        super(DecoderLSTM,self).__init__()\n","        self.size_input=size_input\n","        self.size_embbeding=size_embbeding\n","        self.size_hidden=size_hidden\n","        self.layers=layers\n","        self.size_output=size_output\n","        self.dropout=nn.Dropout(p)\n","        self.tag=True\n","\n","        self.embbed_layer=nn.Embedding(self.size_input,self.size_embbeding)\n","        self.lstm=nn.LSTM(self.size_embbeding,self.size_hidden,self.layers,dropout=p)\n","        self.fc=nn.Linear(self.size_hidden,self.size_output)\n","\n","    def forward(self,x,hidden_st,cell_st):\n","        x=x.unsqueeze(0)\n","        embbeding=self.dropout(self.embbed_layer(x))\n","        outputs, (hidden_st, cell_st) = self.lstm(embbeding, (hidden_st,cell_st))\n","        preds=self.fc(outputs)\n","        preds=preds.squeeze(0)\n","        return preds,hidden_st,cell_st"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dg50X6h0-G5I"},"source":["### Encoder-Decoder Interface Implementation"]},{"cell_type":"code","metadata":{"id":"awKpXwws-Bxq","executionInfo":{"status":"ok","timestamp":1619499674111,"user_tz":-330,"elapsed":105203,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["class Seq2seq_model(nn.Module):\n","    def __init__(self,encoder_net,decoder_net):\n","        super(Seq2seq_model,self).__init__()\n","        self.encoder_net=encoder_net\n","        self.decoder_net=decoder_net\n","\n","    def forward(self,src,target,teacher_forcing=0.5):\n","        batch_length=src.shape[1]\n","        target_len=target.shape[0]\n","        target_vocab_len=eng_lang.num_of_words\n","\n","        outputs=torch.zeros(target_len,batch_length,target_vocab_len).to(device)\n","        hidden_st_enc, cell_st_enc=self.encoder_net(src)\n","        x=target[0]\n","\n","        for i in range(1,target_len):\n","            output,hidden_st_dec,cell_st_dec=self.decoder_net(x,hidden_st_enc,cell_st_enc)\n","            outputs[i]=output\n","            pred=output.argmax(1)\n","            x=target[i] if random.random()<teacher_forcing else pred\n","\n","        return outputs"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sdP2kNSK-XfM"},"source":["**Creating objects of Encoder, Decoder and Seq2Seq model**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XtNautt--Sz1","executionInfo":{"status":"ok","timestamp":1619499674112,"user_tz":-330,"elapsed":105195,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}},"outputId":"dbeab1a4-7eec-4d68-9b78-dde25253c04e"},"source":["encoder_ip_size=hindi_lang.num_of_words\n","encoder_embbeding_size=300\n","encoder_hidden_size=1024\n","encoder_layers=2\n","encoder_dropout=float(0.5)\n","\n","encoder_net=EncoderLSTM(encoder_ip_size, encoder_embbeding_size, encoder_hidden_size, encoder_layers,\n","                        encoder_dropout).to(device)\n","\n","print(encoder_net)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["EncoderLSTM(\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (embbed_layer): Embedding(36160, 300)\n","  (lstm): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P1ohA3qf-mp0","executionInfo":{"status":"ok","timestamp":1619499675040,"user_tz":-330,"elapsed":106117,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}},"outputId":"9af1b9da-c27b-4aee-95dd-7c77270e8275"},"source":["decoder_ip_size=eng_lang.num_of_words\n","decoder_embbed_size=300\n","decoder_hidden_size=1024\n","decoder_layers=2\n","decoder_dropout=float(0.5)\n","decoder_op_size=eng_lang.num_of_words\n","\n","decoder_net=DecoderLSTM(decoder_ip_size,decoder_embbed_size,decoder_hidden_size,\n","                        decoder_layers, decoder_dropout, decoder_op_size).to(device)\n","\n","print(decoder_net)"],"execution_count":29,"outputs":[{"output_type":"stream","text":["DecoderLSTM(\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (embbed_layer): Embedding(25527, 300)\n","  (lstm): LSTM(300, 1024, num_layers=2, dropout=0.5)\n","  (fc): Linear(in_features=1024, out_features=25527, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N_Dk-rV5-pZj","executionInfo":{"status":"ok","timestamp":1619499675041,"user_tz":-330,"elapsed":106109,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}},"outputId":"1dd194c0-0351-42ed-b84c-254426f6b420"},"source":["model=Seq2seq_model(encoder_net, decoder_net)\n","print(model)"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Seq2seq_model(\n","  (encoder_net): EncoderLSTM(\n","    (dropout): Dropout(p=0.5, inplace=False)\n","    (embbed_layer): Embedding(36160, 300)\n","    (lstm): LSTM(300, 1024, num_layers=2, dropout=0.5)\n","  )\n","  (decoder_net): DecoderLSTM(\n","    (dropout): Dropout(p=0.5, inplace=False)\n","    (embbed_layer): Embedding(25527, 300)\n","    (lstm): LSTM(300, 1024, num_layers=2, dropout=0.5)\n","    (fc): Linear(in_features=1024, out_features=25527, bias=True)\n","  )\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RFfBOu2Z-4ce"},"source":["## Setting the training of model"]},{"cell_type":"markdown","metadata":{"id":"Z1Rf16LkVVLK"},"source":["**Note:** To train the model again, please set train_model=True"]},{"cell_type":"code","metadata":{"id":"EZFRwrSmsPJH","executionInfo":{"status":"ok","timestamp":1619499675042,"user_tz":-330,"elapsed":106102,"user":{"displayName":"Shivam Aggarwal","photoUrl":"","userId":"16913159390461513930"}}},"source":["model_available=False"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"UPSKWbhl-sW-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"913cfa31-5105-46ae-e6f6-33d8ae9eaec8"},"source":["batch_size=64\n","optimizer=optim.Adagrad(model.parameters(),lr=0.01)\n","PATH=\"/content/drive/MyDrive/Colab Notebooks/phase1_v2.pth\"\n","\n","epochs=50\n","epoch_loss=0.0\n","padding_idx=eng_lang.word_to_index[\"<PAD>\"]\n","criterion=nn.CrossEntropyLoss(ignore_index=padding_idx)\n","\n","train_model=True\n","\n","if train_model==False:\n","    model=torch.load(PATH)\n","else:\n","    if model_available==True:\n","        model=torch.load(PATH)\n","    batches=len(pairs)//batch_size\n","    for epoch in range(epochs):\n","        print(f\"epoch {epoch+1}/{epochs}\")\n","        model.eval()\n","        model.train(True)\n","        cur_batch=0\n","        for idx in range(0,len(pairs),batch_size):\n","            cur_batch+=1\n","            if(cur_batch%100==0):\n","                print(f\"    running batch {cur_batch} of {batches}\")\n","            if idx+batch_size < len(pairs):\n","                src_batch=hin_tensors[idx:idx+batch_size]\n","                target_batch=eng_tensors[idx:idx+batch_size]\n","            else:\n","                src_batch=hin_tensors[idx:]\n","                target_batch=eng_tensors[idx:]\n","\n","            src_batch=torch.cat(src_batch,dim=1)     #max_len*batch_size\n","            target_batch=torch.cat(target_batch,dim=1)\n","\n","            output=model(src_batch,target_batch)\n","            output=output[1:].reshape(-1,output.shape[2])\n","            target=target_batch[1:].reshape(-1)\n","\n","            optimizer.zero_grad()\n","            loss=criterion(output,target)\n","\n","            loss.backward()\n","\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","        \n","        print(f\"Epoch loss : {loss.item()}\")\n","        \n","        torch.save(model,PATH)\n","        model_available=True"],"execution_count":null,"outputs":[{"output_type":"stream","text":["epoch 1/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 5.565928936004639\n","epoch 2/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 5.020123481750488\n","epoch 3/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 4.999400615692139\n","epoch 4/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 4.763956069946289\n","epoch 5/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 4.5617995262146\n","epoch 6/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 4.589117050170898\n","epoch 7/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 4.277854919433594\n","epoch 8/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 4.196549415588379\n","epoch 9/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 4.089159965515137\n","epoch 10/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 3.9188332557678223\n","epoch 11/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 4.103384017944336\n","epoch 12/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 4.122199535369873\n","epoch 13/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 4.004178524017334\n","epoch 14/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 3.804337978363037\n","epoch 15/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 3.705003261566162\n","epoch 16/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 3.6200616359710693\n","epoch 17/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 3.44946026802063\n","epoch 18/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 3.582982063293457\n","epoch 19/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 3.595491647720337\n","epoch 20/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 3.555318832397461\n","epoch 21/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 3.8126652240753174\n","epoch 22/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 3.7226803302764893\n","epoch 23/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 3.674614906311035\n","epoch 24/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 3.5147926807403564\n","epoch 25/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 3.4477357864379883\n","epoch 26/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 3.4077634811401367\n","epoch 27/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 3.26310658454895\n","epoch 28/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 3.156601905822754\n","epoch 29/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 3.3953635692596436\n","epoch 30/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 3.5296809673309326\n","epoch 31/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 3.283583641052246\n","epoch 32/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 3.126976728439331\n","epoch 33/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 3.486928701400757\n","epoch 34/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 3.0858867168426514\n","epoch 35/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 3.1155197620391846\n","epoch 36/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 3.2348082065582275\n","epoch 37/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 3.5160434246063232\n","epoch 38/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n","    running batch 800 of 1360\n","    running batch 900 of 1360\n","    running batch 1000 of 1360\n","    running batch 1100 of 1360\n","    running batch 1200 of 1360\n","    running batch 1300 of 1360\n","Epoch loss : 3.235191583633423\n","epoch 39/50\n","    running batch 100 of 1360\n","    running batch 200 of 1360\n","    running batch 300 of 1360\n","    running batch 400 of 1360\n","    running batch 500 of 1360\n","    running batch 600 of 1360\n","    running batch 700 of 1360\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ru02atEp_qPy"},"source":["def clean_sentence(sentence):\n","    punctuations=list(string.punctuation)\n","    cleaned=\"\"\n","    for letter in sentence:\n","        if letter=='<' or letter=='>' or letter not in punctuations:\n","            cleaned+=letter\n","    return cleaned  \n","\n","def predict_translation(model,sentence,device,max_length=MAX_LENGTH):\n","    sentence=clean_sentence(sentence)\n","    tokens=sentence.split(\" \")\n","    indexes=[]\n","    for token in tokens:\n","        if token in hindi_lang.word_to_index:\n","            indexes.append(hindi_lang.word_to_index[token])\n","        else:\n","            indexes.append(hindi_lang.word_to_index[\"<UKN>\"])\n","    tensor_of_sentence=torch.LongTensor(indexes).unsqueeze(1).to(device)\n","    with torch.no_grad():\n","        hidden,cell=model.encoder_net(tensor_of_sentence)\n","    outputs=[SOS_token]\n","    for _ in range(max_length):\n","        prev_word=torch.LongTensor([outputs[-1]]).to(device)\n","        with torch.no_grad():\n","            output,hidden,cell=model.decoder_net(prev_word, hidden,cell)\n","            pred=output.argmax(1).item()\n","\n","        outputs.append(pred)\n","\n","        if eng_lang.index_to_word[pred] ==\"<EOS>\":\n","            break\n","    \n","    final=[]\n","\n","    for i in outputs:\n","        if i == \"<PAD>\":\n","            break\n","        final.append(i)\n","\n","    final = [eng_lang.index_to_word[idx] for idx in final]\n","    translated=\" \".join(final)\n","    return translated"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yyfccb0G_W3J"},"source":["test_sentences=[pair[0] for pair in pairs[50:100]]\n","actual_sentences=[pair[1] for pair in pairs[50:100]]\n","pred_sentences=[]\n","\n","for idx,i in enumerate(test_sentences):\n","    translated=predict_translation(model,i,device)\n","    print(\"*\"*20)\n","    print(f\"Hindi: {i}\")\n","    print(f\"Actual: {actual_sentences[idx]}\")\n","    print(f\"Predicted: {translated}\")\n","    print(\"*\"*20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WV4efD7qFC-A"},"source":["## Generating Validation Set results"]},{"cell_type":"code","metadata":{"id":"F-nI_CtCD9su"},"source":["val_data=pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/testhindistatements.csv\")\n","val_data.head()\n","sentences=val_data['hindi']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9b6gixGPFQvJ"},"source":["sentences=sentences.apply(lambda x : x.strip())\n","sentences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N4pX-uzoFZ2i"},"source":["fp=open(\"/content/drive/MyDrive/Colab Notebooks/answer_week1_test.txt\",\"w\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X7Kab1G3Fm4B"},"source":["count=1\n","for sentence in sentences:\n","    translated=predict_translation(model,sentence,device)\n","    translated=translated.split(\" \")[1:-1]\n","    translated=\" \".join(translated)\n","    fp.write(translated+'\\n')\n","    print(f\"sentence : {count}\")\n","    count+=1\n","fp.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kU8oLtbSGRHp"},"source":[""],"execution_count":null,"outputs":[]}]}